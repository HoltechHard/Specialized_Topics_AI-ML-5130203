{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LABORATORY 03: MACHINE LEARNING I - REGRESSION PROBLEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case of Study 1: Predicting Melbourne Housing Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = pd.read_csv('dataset/melbourne_housing.csv', sep = ',')\n",
    "metadata = dataset.columns\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of dataset\n",
    "print(\"#samples = \", dataset.shape[0])\n",
    "print(\"#features = \", dataset.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manage metadata\n",
    "def get_metadata(data):\n",
    "    metadata = data.columns\n",
    "    numerical_cols = data.select_dtypes(include = [\"float64\", \"int64\"]).columns.tolist()\n",
    "    categorical_cols = data.select_dtypes(include = [\"object\"]).columns.tolist()\n",
    "    print(\"Numerical features: \", numerical_cols)\n",
    "    print(\"Categorical features: \", categorical_cols)\n",
    "    return metadata, numerical_cols, categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata, numeric_cols, categ_cols = get_metadata(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter missing data\n",
    "def filter_missing(data):\n",
    "    sbn.displot(\n",
    "        data = data.isna().melt(value_name=\"missing\"),\n",
    "        y = \"variable\",\n",
    "        hue = \"missing\",\n",
    "        multiple = \"fill\",\n",
    "        aspect = 1.5\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original state of missing values\n",
    "filter_missing(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot histogram of frequencies\n",
    "def hist_frequencies(data, numeric_cols, bins):\n",
    "    # calculate the nrows and ncols for plots\n",
    "    ncol_plots = 3\n",
    "    nrow_plots = (len(numeric_cols) + ncol_plots - 1) // ncol_plots\n",
    "    # create the subplots for specific row and column\n",
    "    fig, axs = plt.subplots(nrow_plots, ncol_plots, figsize = (16, 4 * nrow_plots))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        sbn.histplot(data[col], color = \"blue\", bins = bins, ax = axs[i])\n",
    "        axs[i].set_title(\"Histogram of frequencies for \" + col)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Frequencies\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_frequencies(dataset, numeric_cols, bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical metrics\n",
    "display(dataset[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle to calculate number of instances in each categorical column\n",
    "for col in categ_cols:\n",
    "    print(\"\\n***** \" + col + \" ******\")\n",
    "    print(dataset[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns which don't have any relevance\n",
    "def feature_engineer(data):\n",
    "    # extract information from Date    \n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"], errors = \"coerce\")\n",
    "    data[\"SaleDayOfWeek\"] = data[\"Date\"].dt.dayofweek\n",
    "    \n",
    "    # drop non-relevant columns \n",
    "    nrelev_cols = [\"Address\", \"BuildingArea\", \"YearBuilt\", \"Date\", \"SellerG\"]\n",
    "    data = data.drop(nrelev_cols, axis = 1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = feature_engineer(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#samples = \", dataset.shape[0])\n",
    "print(\"#features = \", dataset.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata, numeric_cols, categ_cols = get_metadata(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_missing(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data imputation with K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def imputation_data(data, num_cols, categ_cols):\n",
    "    # imputation for numerical columns\n",
    "    knn_imputer = KNNImputer(n_neighbors = 5)\n",
    "    data[num_cols] = knn_imputer.fit_transform(data[num_cols])\n",
    "    # imputation for categorical columns\n",
    "    for col in categ_cols:\n",
    "        data[col] = data[col].fillna(data[col].mode()[0])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = imputation_data(dataset, numeric_cols, categ_cols)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state of missing values after imputation\n",
    "filter_missing(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration of Data after Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check type of relationship between variables\n",
    "def gen_pairplot(data, metadata):\n",
    "    sbn.set_theme(context = 'notebook', style = 'darkgrid')    \n",
    "    sbn.pairplot(data[metadata], height = 2.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_pairplot(dataset, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot correlation between numerical variables\n",
    "def plot_correlation(data, cols):\n",
    "    corr = data[cols].corr()\n",
    "    plt.matshow(corr, cmap = \"coolwarm\")\n",
    "    plt.xticks(range(len(cols)), cols, rotation = 90)\n",
    "    plt.yticks(range(len(cols)), cols)\n",
    "\n",
    "    # add the correlation values in each cell\n",
    "    for (i, j), val in np.ndenumerate(corr):\n",
    "        plt.text(j, i, f\"{val:.1f}\", ha='center', va='center', color='black')\n",
    "    plt.title(\"Correlation Analysis\")\n",
    "    plt.colorbar()    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation(dataset, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability distribution of the target variable\n",
    "plt.figure(figsize=[8,4])\n",
    "sbn.histplot(dataset[\"Price\"], color='g', edgecolor=\"black\", linewidth=2, bins=20)\n",
    "\n",
    "plt.title(\"Target Variable Distribution - Median Value of Homes ($1Ms)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split independent and dependent variables\n",
    "x = dataset.loc[:, dataset.columns != \"Price\"]\n",
    "y = dataset[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension of features = \", x.shape)\n",
    "print(\"Dimension of target = \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"X-train dim: \", x_train.shape)\n",
    "print(\"Y-train: \", len(y_train))\n",
    "print(\"X-test dim: \", x_test.shape)\n",
    "print(\"Y-test: \", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Always: split the data into training and test set, then apply preprocessing!!! <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the numerical and categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take metadata\n",
    "metadata, numeric_cols, categ_cols = get_metadata(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Criteria to scale numerical features<b> </center>   \n",
    "\n",
    "**Standard Scaler**  \n",
    "\n",
    "$$X' = \\frac{X - \\mu}{\\sigma}$$\n",
    "  \n",
    "where:  \n",
    "  \n",
    "* $X$ is the original feature value  \n",
    "* $X'$ is the scaled feature value  \n",
    "* $\\mu$ is the mean of the feature values  \n",
    "* $\\sigma$ is the standard deviation of the feature values  \n",
    "  \n",
    "**Robust Scaler**  \n",
    "  \n",
    "$$X' = \\frac{X - Q_1}{Q_3 - Q_1}$$  \n",
    "  \n",
    "where:  \n",
    "  \n",
    "* $X$ is the original feature value  \n",
    "* $X'$ is the scaled feature value  \n",
    "* $Q_1$ is the first quartile (25th percentile) of the feature values  \n",
    "* $Q_3$ is the third quartile (75th percentile) of the feature values  \n",
    "  \n",
    "Note: The Robust Scaler uses the interquartile range (IQR) instead of the standard deviation to make it more robust to outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "transformer = make_column_transformer(\n",
    "    (StandardScaler(), [\"Distance\", \"Lattitude\", \"Longtitude\", \"Propertycount\"]),  \n",
    "    (RobustScaler(), [\"Rooms\", \"Postcode\", \"Bedroom2\", \"Bathroom\", \"Car\", \"Landsize\"]),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), [\"SaleDayOfWeek\", \"Type\", \"Method\", \"Regionname\"]),\n",
    "    (OrdinalEncoder(\n",
    "        categories = [x.Suburb.unique(), x.CouncilArea.unique()], dtype = np.int64), \n",
    "        [\"Suburb\", \"CouncilArea\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer will learn only from training data\n",
    "transformer.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer will transform the train and test data\n",
    "x_train = transformer.transform(x_train)\n",
    "x_test = transformer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark for models:\n",
    "\n",
    "* XGBoost Regressor\n",
    "* LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# function to save model\n",
    "def save_model(filename, model):\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load model\n",
    "def load_model(filename):\n",
    "    with open(filename, \"rb\") as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <b>Criteria to evaluate quality of model<b> </center>  \n",
    "  \n",
    "**RMSE (Root Mean Squared Error)**\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $y_i$ is the actual value of the $i^{th}$ observation\n",
    "* $\\hat{y}_i$ is the predicted value of the $i^{th}$ observation\n",
    "* $n$ is the total number of observations\n",
    "\n",
    "**R2 (Coefficient of Determination)**\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $y_i$ is the actual value of the $i^{th}$ observation\n",
    "* $\\hat{y}_i$ is the predicted value of the $i^{th}$ observation\n",
    "* $\\bar{y}$ is the mean of the actual values\n",
    "* $n$ is the total number of observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to evaluate model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def eval_model_perform(model, x, y):    \n",
    "    y_pred = model.predict(x)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    r2_val = r2_score(y, y_pred)\n",
    "\n",
    "    return rmse_val, r2_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1. XGBoost Regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# hyperparameters definition\n",
    "xgb_params = {          \n",
    "    \"max_depth\": [8, 16, 32],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.7, 0.8],\n",
    "    \"colsample_bytree\": [0.8, 0.9],\n",
    "    \"tree_method\": [\"hist\"],\n",
    "    \"objective\": [\"reg:squarederror\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define XGB Model\n",
    "def XGBModel(x_train, y_train, params):\n",
    "    # define the model    \n",
    "    model = xgb.XGBRegressor()\n",
    "    \n",
    "    # hyperparameter optimization\n",
    "    grid_search = GridSearchCV(estimator = model,\n",
    "                               param_grid = params,\n",
    "                               scoring = \"neg_mean_squared_error\",\n",
    "                               cv = 5,\n",
    "                               n_jobs = -1\n",
    "                            )\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    \n",
    "    # get best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "sttrain_xgb = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBModel(x_train, y_train, xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ettrain_xgb = time.time()\n",
    "ttrain_xgb = ettrain_xgb - sttrain_xgb\n",
    "print(f\"Time of training: {ttrain_xgb:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save xgb model\n",
    "save_model(\"models/xgb_v1.pkl\", xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover the model\n",
    "rec_xgb = load_model(\"models/xgb_v1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics for train set\n",
    "rmse_xgb_train, r2_xgb_train = eval_model_perform(rec_xgb, x_train, y_train)\n",
    "print(f\"R-MSE train score: {rmse_xgb_train:.3f}\")\n",
    "print(f\"R^2 train score: {r2_xgb_train:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2-score for test set\n",
    "rmse_xgb_test, r2_xgb_test = eval_model_perform(rec_xgb, x_test, y_test)\n",
    "print(f\"R-MSE test score: {rmse_xgb_test:.3f}\")\n",
    "print(f\"R^2 test score: {r2_xgb_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2. Light Gradient Boost Machine (LightGBM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import lightgbm as lgbm\n",
    "\n",
    "# define grid hyperparameters\n",
    "lgbm_params = {    \n",
    "    \"num_leaves\": [64, 128, 256],\n",
    "    \"max_depth\": [10, 20, 40],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.8, 0.9],\n",
    "    \"subsample_freq\": [10] # re-sample without replacement every 10 iterations\n",
    "                         # and extract bagging_fraction% of training data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LightGBM regressor\n",
    "def LightGBModel(x_train, y_train, params):\n",
    "    lgbm_model = lgbm.LGBMRegressor()\n",
    "    \n",
    "    # hyperparameter optimization\n",
    "    grid_lgbm = GridSearchCV(estimator = lgbm_model,  # regressor model\n",
    "                         param_grid = params,  # dict of hyperparameters\n",
    "                         cv = 5,   # 5-fold cross-validation\n",
    "                         scoring = \"r2\",\n",
    "                         verbose = False,\n",
    "                         n_jobs = -1\n",
    "                    )\n",
    "    # fit the model\n",
    "    grid_lgbm.fit(x_train, y_train)\n",
    "    \n",
    "    # take best model\n",
    "    best_model = grid_lgbm.best_estimator_\n",
    "    print(grid_lgbm.best_params_)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "sttrain_lgbm = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the best model\n",
    "lgbm_model = LightGBModel(x_train, y_train, lgbm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ettrain_lgbm = time.time()\n",
    "ttrain_lgbm = ettrain_lgbm - sttrain_lgbm\n",
    "print(f\"Time of training: {ttrain_lgbm:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save xgb model\n",
    "save_model(\"models/lgbm_v1.pkl\", lgbm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover the model\n",
    "rec_lgbm = load_model(\"models/lgbm_v1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics for train set\n",
    "rmse_lgbm_train, r2_lgbm_train = eval_model_perform(rec_lgbm, x_train, y_train)\n",
    "print(f\"R-MSE train score: {rmse_lgbm_train:.3f}\")\n",
    "print(f\"R^2 train score: {r2_lgbm_train:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2-score for test set\n",
    "rmse_lgbm_test, r2_lgbm_test = eval_model_perform(rec_lgbm, x_test, y_test)\n",
    "print(f\"R-MSE test score: {rmse_lgbm_test:.3f}\")\n",
    "print(f\"R^2 test score: {r2_lgbm_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Monitoring results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_res = {\n",
    "    \"xgboost\": pd.DataFrame({\n",
    "        \"train\": {\"rmse\": rmse_xgb_train, \"r2\": r2_xgb_train},\n",
    "        \"test\": {\"rmse\": rmse_xgb_test, \"r2\": r2_xgb_test}        \n",
    "    }),\n",
    "    \"lgbm\": pd.DataFrame({\n",
    "        \"train\": {\"rmse\": rmse_lgbm_train, \"r2\": r2_lgbm_train},\n",
    "        \"test\": {\"rmse\": rmse_lgbm_test, \"r2\": r2_lgbm_test}        \n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, res in dict_res.items():\n",
    "    print(f\"\\nModel: {key}\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_reg_results(res):\n",
    "    # Create a figure with two subplots\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Iterate over the dictionary and plot the results for each model\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    for i, (key, res) in enumerate(dict_res.items()):\n",
    "        # Plot the RMSE values\n",
    "        sns.barplot(x=[f\"{key} Train\", f\"{key} Test\"], y=[res[\"train\"][\"rmse\"], res[\"test\"][\"rmse\"]], ax=ax[0])\n",
    "        # Plot the R2 values\n",
    "        sns.barplot(x=[f\"{key} Train\", f\"{key} Test\"], y=[res[\"train\"][\"r2\"], res[\"test\"][\"r2\"]], ax=ax[1])\n",
    "\n",
    "    # Set the titles and labels for the subplots\n",
    "    ax[0].set_title(\"RMSE\")\n",
    "    ax[0].set_xlabel(\"Model\")\n",
    "    ax[0].set_ylabel(\"RMSE\")\n",
    "\n",
    "    ax[1].set_title(\"R2\")\n",
    "    ax[1].set_xlabel(\"Model\")\n",
    "    ax[1].set_ylabel(\"R2\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitoring the results\n",
    "plot_reg_results(dict_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
