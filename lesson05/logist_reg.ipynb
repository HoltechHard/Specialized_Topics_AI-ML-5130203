{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv(\"dataset/data.csv\", header=None)\n",
    "X = np.array(data[[0,1]])\n",
    "y = np.array(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot ploints\n",
    "def plot_points(X, y):\n",
    "    admitted = X[np.argwhere(y==1)]\n",
    "    rejected = X[np.argwhere(y==0)]\n",
    "    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')\n",
    "    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')\n",
    "    plt.title(\"Distribution of Dataset for logistic regression\")\n",
    "    plt.xlabel(\"X1 axis\")\n",
    "    plt.ylabel(\"X2 axis\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Implementing the basic functions\n",
    "\n",
    "- Sigmoid activation function\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "- Output (prediction) formula: Linear combination\n",
    "\n",
    "$$\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + b)$$\n",
    "\n",
    "- Error function: Cross entropy Loss\n",
    "\n",
    "$$L(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$$\n",
    "\n",
    "- The function that updates the weights\n",
    "\n",
    "$$ w_i \\longleftarrow w_i + \\eta (y - \\hat{y}) x_i$$\n",
    "\n",
    "$$ b \\longleftarrow b + \\eta (y - \\hat{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def output_formula(features, weights, bias):\n",
    "    return sigmoid(np.dot(features, weights) + bias)\n",
    "\n",
    "def error_formula(y, output):\n",
    "    return - y * np.log(output) - (1 - y) * np.log(1 - output)\n",
    "\n",
    "def update_weights(x, y, weights, bias, learnrate):\n",
    "    output = output_formula(x, weights, bias)\n",
    "    d_error = -(y - output)\n",
    "    weights -= learnrate * d_error * x\n",
    "    bias -= learnrate * d_error\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision boundary\n",
    "def display(m, b, color='g--'):\n",
    "    plt.xlim(-0.05,1.05)\n",
    "    plt.ylim(-0.05,1.05)\n",
    "    x = np.arange(-10, 10, 0.1)\n",
    "    plt.plot(x, m*x+b, color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# define #iterations and learning rate\n",
    "epochs = 100\n",
    "learnrate = 0.01\n",
    "\n",
    "# function to train model\n",
    "def train(features, targets, epochs, learnrate, graph_lines=False):    \n",
    "    errors = []\n",
    "    n_samples, n_features = features.shape\n",
    "    last_loss = None\n",
    "\n",
    "    # initialize weights and biases \n",
    "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "    bias = 0\n",
    "\n",
    "    # iterative process of training\n",
    "    for e in range(epochs):\n",
    "        # delta_w\n",
    "        del_w = np.zeros(weights.shape)\n",
    "\n",
    "        # gradient descencent algorithm\n",
    "        for x, y in zip(features, targets):\n",
    "            output = output_formula(x, weights, bias)\n",
    "            error = error_formula(y, output)\n",
    "            weights, bias = update_weights(x, y, weights, bias, learnrate)\n",
    "        \n",
    "        # calculate final loss by epoch\n",
    "        out = output_formula(features, weights, bias)\n",
    "        loss = np.mean(error_formula(targets, out))\n",
    "        errors.append(loss)\n",
    "\n",
    "        # show results\n",
    "        if e % (epochs / 10) == 0:\n",
    "            print(\"\\n========== Epoch\", e,\"==========\")\n",
    "            if last_loss and last_loss < loss:\n",
    "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "            else:\n",
    "                print(\"Train loss: \", loss)\n",
    "            last_loss = loss\n",
    "            # check treshold\n",
    "            predictions = out > 0.5            \n",
    "            # calculate accuracy\n",
    "            accuracy = np.mean(predictions == targets)\n",
    "            print(\"Accuracy: \", accuracy)\n",
    "\n",
    "        if graph_lines and e % (epochs / 100) == 0:\n",
    "            display(-weights[0]/weights[1], -bias/weights[1])\n",
    "            \n",
    "\n",
    "    # Plotting the solution boundary\n",
    "    plt.title(\"Solution boundary\")\n",
    "    display(-weights[0]/weights[1], -bias/weights[1], 'black')\n",
    "\n",
    "    # Plotting the data\n",
    "    plot_points(features, targets)\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the error\n",
    "    plt.title(\"Error Plot\")\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.ylabel('Error')\n",
    "    plt.plot(errors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(X, y, epochs, learnrate, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
